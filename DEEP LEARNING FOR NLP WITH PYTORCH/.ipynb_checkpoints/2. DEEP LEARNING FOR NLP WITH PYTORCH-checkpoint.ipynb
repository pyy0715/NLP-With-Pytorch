{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본: https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#DEEP-LEARNING-FOR-NLP-WITH-PYTORCH\" data-toc-modified-id=\"DEEP-LEARNING-FOR-NLP-WITH-PYTORCH-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>DEEP LEARNING FOR NLP WITH PYTORCH</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deep-Learning-Building-Blocks:-Affine-maps,-non-linearities-and-objectives\" data-toc-modified-id=\"Deep-Learning-Building-Blocks:-Affine-maps,-non-linearities-and-objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Deep Learning Building Blocks: Affine maps, non-linearities and objectives</a></span></li><li><span><a href=\"#Affine-Maps\" data-toc-modified-id=\"Affine-Maps-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Affine Maps</a></span></li><li><span><a href=\"#Non-Linearities\" data-toc-modified-id=\"Non-Linearities-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Non-Linearities</a></span></li><li><span><a href=\"#Softmax-and-Probabilities\" data-toc-modified-id=\"Softmax-and-Probabilities-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Softmax and Probabilities</a></span></li><li><span><a href=\"#Objective-Functions\" data-toc-modified-id=\"Objective-Functions-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Objective Functions</a></span></li><li><span><a href=\"#Optimization-and-Training\" data-toc-modified-id=\"Optimization-and-Training-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Optimization and Training</a></span></li><li><span><a href=\"#Creating-Network-Components-in-PyTorch\" data-toc-modified-id=\"Creating-Network-Components-in-PyTorch-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Creating Network Components in PyTorch</a></span></li><li><span><a href=\"#Example:-Logistic-Regression-Bag-of-Words-classifier\" data-toc-modified-id=\"Example:-Logistic-Regression-Bag-of-Words-classifier-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Example: Logistic Regression Bag-of-Words classifier</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING FOR NLP WITH PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Building Blocks: Affine maps, non-linearities and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝은 영리한 방법으로 비선형성을 가진 선형성을 구성하는 것으로 이루어집니다. \n",
    "\n",
    "비선형성의 도입은 강력한 모델을 가능하게 합니다. \n",
    "\n",
    "이 섹션에서 이 핵심 구성 요소를 다루고, 객체 함수를 만들고, 어떻게 모델이 학습되지는 살펴봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝의 핵심 요소 중 하나는 affine map입니다. \n",
    "\n",
    "이것은 함수 f (x)입니다.\n",
    "\n",
    "$$f(x) = Ax + b$$\n",
    "\n",
    "행렬 A 및 벡터 x, b에 대해. 여기서 학습해야 할 모수는 A와 b입니다. 종종 b는 편항이라고합니다.\n",
    "\n",
    "PyTorch 및 대부분의 다른 딥 러닝 프레임 워크는 기존 선형 대수와 약간 다르게 작동합니다. \n",
    "\n",
    "열 대신 입력 행을 매핑합니다. \n",
    "\n",
    "즉, 아래 출력의 i 번째 행은 A의 i 번째 행과 편항을 매핑 한 것입니다.\n",
    "\n",
    "아래 예를보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T05:59:24.247857Z",
     "start_time": "2019-11-17T05:59:23.335764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c1801dc1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T05:59:24.904843Z",
     "start_time": "2019-11-17T05:59:24.862320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1755, -0.3268, -0.5069],\n",
      "        [-0.6602,  0.2260,  0.1089]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)  # maps from R^5 to R^3, parameters A, b\n",
    "# data is 2x5.  A maps from 5 to 3... can we map \"data\" under A?\n",
    "data = torch.randn(2, 5)\n",
    "print(lin(data))  # yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 다음 사실을 주목하십시오. \n",
    "\n",
    "왜냐하면 처음에 비선형성이 왜 필요한지 설명 할 것입니다. \n",
    "\n",
    "두 개의 affin map f(x) = Ax + b와 g(x) = Cx + d가 있다고 가정합니다. f(g(x)) 란 무엇입니까?\n",
    "\n",
    "f(g(x))=A(Cx+d)+b=ACx+(Ad+b)\n",
    "\n",
    "AC는 행렬이고 Ad + b는 벡터이므로 affine map을 구성하면 affine map을 얻을 수 있습니다.\n",
    "\n",
    "affine layer 사이에 비선형성을 도입하면 훨씬 더 강력한 모델을 만들 수 있습니다.\n",
    "\n",
    "몇 가지 핵심적인 비선형성이 있습니다. tanh(x), σ(x), ReLU(x)가 가장 일반적입니다. \n",
    "\n",
    "아마도 궁금 할 것입니다. \"이러한 기능이 왜 작동합니까? 다른 많은 비선형 성도 생각할 수 있습니다.\"\n",
    "\n",
    "그 이유는 계산하기 쉬운 그라디언트가 있고 그라디언트 계산이 학습에 필수적이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:30:23.434807Z",
     "start_time": "2019-11-17T06:30:23.422797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5404, -2.2102],\n",
      "        [ 2.1130, -0.0040]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [2.1130, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = torch.randn(2, 2)\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax(x) 함수는 비선형성을 가질 뿐만 아니라 일반적으로 네트워크에서 마지막으로 수행된다는 점에서 특별합니다. \n",
    "\n",
    "실수로 구성된 벡터를 받아서 확률 분포로 반환하기 때문입니다.\n",
    "\n",
    "그런 다음 Softmax(x)의 i 번째 구성 요소는\n",
    "\n",
    "$$ \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "\n",
    "결과는 확률 분포임이 분명해야합니다. 각 요소는 음수가 아니고 모든 구성 요소의 합은 1입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:40:39.196206Z",
     "start_time": "2019-11-17T06:40:39.177681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])\n",
      "tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])\n",
      "tensor(1.)\n",
      "tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "print(F.softmax(data, dim=0))\n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "print(F.log_softmax(data, dim=0))  # theres also log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적함수는 네트워크가 무엇인가를 최소화하도록 훈련되는 함수입니다(이 경우 손실함수 또는 비용함수라고도 함).\n",
    "\n",
    "먼저 훈련 인스턴스를 선택하고 신경망을 통해 실행 한 다음 출력의 손실을 계산합니다.\n",
    "\n",
    "그 다음, 손실함수에 미분을 취하여 모델의 파라미터를 업데이트합니다.\n",
    "\n",
    "직관적으로 모델이 답변을 확신하지만 만약 답변이 틀리다면 손실이 커질 것입니다. \n",
    "\n",
    "하지만 답변에 확신이 있고 만약 답변이 정확하다면 손실은 감소할 것입니다.\n",
    "\n",
    "손실 함수의 예로는 negative log likelihood loss로, 이는 다중 클래스 분류 문제의 경우에 일반적으로 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 인스턴스에 대한 손실 함수를 계산할 수있는 것은 무엇입니까? \n",
    "\n",
    "우리는 그걸로 무엇을합니까? \n",
    "\n",
    "우리는 텐서가 그라디언트를 계산하는데 사용 된 것들과 관련하여 그라디언트를 계산하는 방법을 알고 있음을 앞에서 보았습니다. \n",
    "\n",
    "우리의 손실은 텐서이므로, 계산에 사용된 모든 파라미터와 관련하여 기울기를 계산할 수 있습니다! \n",
    "\n",
    "그런 다음 그라디언트 업데이트를 수행 할 수 있습니다. \n",
    "\n",
    "θ를 우리의 매개 변수, L(θ) 손실 함수, η을 학습률이라고 합시다. \n",
    "\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta) $$\n",
    "\n",
    "이 바닐라 그래디언트 업데이트 이외의 다른 작업을 시도하는 방대한 알고리즘과 연구들이 있었습니다.\n",
    "\n",
    "하지만 실제로 관심이 없다면 이러한 알고리즘이 무엇을하고 있는지에 대해 걱정할 필요가 없습니다. \n",
    "\n",
    "Torch는 torch.optim 패키지에 많은 것을 제공하며, 명쾌합니다.\n",
    "\n",
    "네트워크의 성능을 최적화하려면 업데이트 알고리즘에 대해 다른 업데이트 알고리즘과 다른 파라미터 (예 : 초기 학습률)를 시도하는 것이 중요합니다. \n",
    "\n",
    "종종 바닐라 SGD를 Adam 또는 RMSProp와 같은 최적화 옵티마이저로 교체하면 성능이 현저하게 향상됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network Components in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP에 초점을 맞추기 전에 affine map과 비선형성만 사용하여 PyTorch에 네트워크를 구축하는 예를 들어 보겠습니다. \n",
    "\n",
    "또한 PyTorch에 내장된 negative log likelihood loss를 사용하여 손실 함수를 계산하고, 역전파를 통해 파라미터를 업데이트하는 방법도 살펴 봅니다.\n",
    "\n",
    "모든 네트워크 구성 요소는 nn.Module에서 상속하고 forward() 메서드를 override 해야합니다.\n",
    "\n",
    "nn.Module에서 상속하여 구성 요소에 기능을 제공합니다.\n",
    "\n",
    "예제로 희소한 bag-of-words 단어를 표현하고 \"English\"와 \"Spanish\"라는 두 레이블에 대한 확률 분포를 출력하는 네트워크를 작성해 봅시다. \n",
    "\n",
    "이 모델은 로지스틱 회귀입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression Bag-of-Words classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 vocab의 각 단어에 index를 할당합니다. \n",
    "\n",
    "예를 들어, 우리의 전체 어휘가 \"hello\"와 \"world\"라는 두 단어이며 각각 index 0과 1을 가지고 있다고 가정하십시오. \n",
    "\n",
    "\"hello hello hello hello\" 문장의 BoW 벡터는 [4,0]이며, \"hello world world hello\"는 [2,2]로 나타낼 수 있습니다. \n",
    "\n",
    "이는 일반적으로 BoW백터를 [Count(hello),Count(world)]로 나타낼 수 있습니다.\n",
    "\n",
    "우리 네트워크의 출력은 다음과 같습니다.\n",
    "\n",
    "$$ \\log \\text{Softmax}(Ax + b) $$\n",
    "\n",
    "즉, 입력을 affin map을 통해 전달한 다음 log softmax를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:47:02.613178Z",
     "start_time": "2019-11-17T07:47:02.597723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
      "Parameter containing:\n",
      "tensor([[ 0.1860, -0.1301,  0.0245,  0.1464,  0.1421,  0.1218, -0.1419, -0.1412,\n",
      "         -0.1186,  0.0246,  0.1955, -0.1239,  0.1045, -0.1085, -0.1844, -0.0417,\n",
      "          0.1130,  0.1821, -0.1218,  0.0426,  0.1692,  0.1300,  0.1222,  0.1394,\n",
      "          0.1240,  0.0507],\n",
      "        [-0.1341, -0.1647, -0.0899, -0.0228, -0.1202,  0.0717,  0.0607, -0.0444,\n",
      "          0.0754,  0.0634,  0.1197,  0.1321, -0.0664,  0.1916, -0.0227, -0.0067,\n",
      "         -0.1851, -0.1262, -0.1146, -0.0839,  0.1394, -0.0641, -0.1466,  0.0755,\n",
      "          0.0628,  0.1270]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1015,  0.0425], requires_grad=True)\n",
      "tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.3691, -1.1756]])\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = data[0]\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    print(bow_vector)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 값 중 ENGLISH, SPANISH의 로그 확률과 일치하는 값은 무엇입니까? \n",
    "\n",
    "우리는 정의하지 않았지만, 훈련시키고 싶다면 어느 것인지 정의를 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:34:16.994141Z",
     "start_time": "2019-11-17T07:34:16.992139Z"
    }
   },
   "outputs": [],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 훈련을 해봅시다!\n",
    "\n",
    "이를 위해 인스턴스를 통해 로그 확률을 얻고 손실 함수를 계산하고 손실 함수의 기울기를 계산 한 다음 단계로 파라미터를 업데이트합니다. \n",
    "\n",
    "손실함수는 Torch의 nn패키지에서 제공합니다. \n",
    "\n",
    "nn.NLLLoss()는 우리가 원하는 negative log likelihood loss입니다. \n",
    "\n",
    "또한 torch.optim에 최적화 기능을 정의합니다. 여기서는 SGD만 사용하겠습니다.\n",
    "\n",
    "NLLLoss에 대한 입력 값은 로그 확률로 구성된 벡터이며 대상 레이블입니다. \n",
    "\n",
    "우리가 직접 로그 확률을 계산하지 않습니다. \n",
    "\n",
    "이것이 네트워크의 마지막 계층이 log softmax인 이유입니다. \n",
    "\n",
    "손실 함수 nn.CrossEntropyLoss ()는 log softmax를 제외하고 NLLLoss ()와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:34:17.539743Z",
     "start_time": "2019-11-17T07:34:17.361172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9297, -0.5020]])\n",
      "tensor([[-0.6388, -0.7506]])\n",
      "tensor([-0.1488, -0.1313], grad_fn=<SelectBackward>)\n",
      "tensor([[-0.2093, -1.6669]])\n",
      "tensor([[-2.5330, -0.0828]])\n",
      "tensor([ 0.2803, -0.5605], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 정답을 얻었습니다! \n",
    "\n",
    "첫 번째 예에서는 스페인어의 로그 확률이 훨씬 높고, 테스트 데이터의 경우 영어의 로그 확률이 두번째로 훨씬 높음을 알 수 있습니다.\n",
    "\n",
    "이제 PyTorch 구성 요소를 작성하고 일부 데이터를 전달하며 그라디언트 업데이트를 수행하는 방법을 살펴 보았습니다. \n",
    "\n",
    "우리는 NLP가 제공하는 것에 대해 더 깊이 파고들 준비가되어 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
