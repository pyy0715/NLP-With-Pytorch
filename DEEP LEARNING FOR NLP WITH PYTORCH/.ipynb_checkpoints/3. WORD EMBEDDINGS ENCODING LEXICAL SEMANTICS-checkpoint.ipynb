{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#WORD-EMBEDDINGS:-ENCODING-LEXICAL-SEMANTICS\" data-toc-modified-id=\"WORD-EMBEDDINGS:-ENCODING-LEXICAL-SEMANTICS-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Getting-Dense-Word-Embeddings\" data-toc-modified-id=\"Getting-Dense-Word-Embeddings-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Getting Dense Word Embeddings</a></span></li><li><span><a href=\"#Word-Embeddings-in-Pytorch\" data-toc-modified-id=\"Word-Embeddings-in-Pytorch-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Word Embeddings in Pytorch</a></span></li><li><span><a href=\"#An-Example:-N-Gram-Language-Modeling\" data-toc-modified-id=\"An-Example:-N-Gram-Language-Modeling-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>An Example: N-Gram Language Modeling</a></span></li><li><span><a href=\"#Exercise:-Computing-Word-Embeddings:-Continuous-Bag-of-Words\" data-toc-modified-id=\"Exercise:-Computing-Word-Embeddings:-Continuous-Bag-of-Words-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD EMBEDDINGS: ENCODING LEXICAL SEMANTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding은 실수(real numbers)로 구성된 dense vector로, 단어장의 단어 하나마다 한 원소를 갖고 있다.\n",
    "\n",
    "NLP 문제를 다루는 대부분의 경우 우리의 feature는 단어일 것이다! \n",
    "\n",
    "하지만 컴퓨터로 어떻게 단어를 표현할 것인가? \n",
    "\n",
    "단어의 ASCII 표현을 사용할 수도 있겠으나, 그것은 단어가 무엇인지 알려줄 뿐, 어떤 의미를 갖고 있는지는 말해주지 않는다.\n",
    "\n",
    "게다가, 단어들의 표현을 어떤 방식으로 조합할 수 있는 지는 더 어려운 문제이다. \n",
    "\n",
    "V가 단어장이고 input이 |V|차원이라면, output의 차원이 더 작은 차원을 가지는 경우 우리는 neural network을 통해서 dense ouput를 원한다.\n",
    "\n",
    "어떻게 하면 엄청나게 큰 차원에서 더 작은 차원을 얻을 수 있을까?\n",
    "\n",
    "ASCII 표현 대신에 원핫인코딩을 사용하는건 어떨까?\n",
    "\n",
    "$$\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}$$ \n",
    "\n",
    "즉, 우리는 위와 같이 단어 w를 표현할 수 있다.\n",
    "\n",
    "여기서 1은 단어 w의 위치를 표시한다. 다른 단어들도 어딘가의 위치에서 1을 가지며, 나머지는 0을 갖게 된다.\n",
    "\n",
    "이 표현 방법은 그 크기가 거대하다는 점 외에 또 엄청난 단점이 있다.\n",
    "\n",
    "그것은 이 표현이 모든 단어들을 서로 간의 관계를 전혀 고려하지 않고 하나 하나 단어만 독립적으로 표현한다는 점이다.\n",
    "\n",
    "우리가 진짜 원하는 것은 단어간의 유사성이다. 왜그럴까? 예제를 살펴보자.\n",
    "\n",
    "언어 모델을 만든다고 가정하고, 아래와 같은 문장을 train 데이터로 받았다고 생각해보자.\n",
    "\n",
    "* The mathematician ran to the store.\n",
    "* The physicist ran to the store.\n",
    "* The mathematician solved the open problem.\n",
    "\n",
    "이제 training 데이터에서 못 봤던 새로운 문장 하나가 들어왔다.\n",
    "\n",
    "* The physicist solved the open problem.\n",
    "\n",
    "우리의 언어 모델이 이 문장에 대해서 괜찮은 결과를 낼 수도 있지만, 아래의 두 가지 사실을 이용한다면 훨씬 그 결과가 좋아질 수 있지 않을까.\n",
    "\n",
    "* 문장 안에서 mathematician과 physicist는 같은 역할을 하고 있다. 아마도 이 둘은 뭔가 의미적으로 관련이 있을 것 같다.\n",
    "* 새로운 문장에서 physicist는 mathematician과 같은 역할을 하고 있다.\n",
    "\n",
    "그럼 우리는 physicist는 실제로 적합한지 추론할 수 있을까?\n",
    "\n",
    "이것이 여기서 이야기 하고자 하는 유사성의 의미이다.\n",
    "\n",
    "비슷한 철자표현이 아닌 의미적인 유사성을 말하는 것이다.\n",
    "\n",
    "이것이 우리가 관측한 것과 모르는 것 사이를 연결해서 언어학적인 데이터의 희소성를 극복하기 위한 기술이다.\n",
    "\n",
    "물론 위 예제는 \"비슷한 문맥에서 등장하는 단어들은 서로 의미적으로 연관되어 있다\"는 언어학적인 기본 전제를 필요로 한다. \n",
    "\n",
    "이를 distributional hypothesis라고 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 이 문제를 어떻게 해결할 수 있을까? \n",
    "\n",
    "즉, 단어 간의 의미적 유사성을 어떻게 측정하고 encode 할 수 있을까? \n",
    "\n",
    "아마도 우리는 의미에 맞는 속성들을 생각해 볼 수도 있다.\n",
    "\n",
    "예를들어, 우리가 mathematicians과 physicists 둘다 달릴 수 있다는 것을 알기 때문에 'is able to run'이란 속성에 단어들을 높은 점수를 부여할 수 있다.\n",
    "\n",
    "다른 속성들을 고려하였을떄, 그 속성들에 대해 어떤 공통적인 단어로 평가할 수 있을지 생각해보자.\n",
    "\n",
    "만약 각각의 속성을 차원이라 하였을떄, 우리는 아래와 같이 단어에 백터를 부여할 수 있다.\n",
    "\n",
    "$$q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
    "\\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]$$\n",
    "\n",
    "$$q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
    "\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]$$\n",
    "\n",
    "그럼 우리는 단어들의 간의 유사성을 측정할 수 있다.\n",
    "\n",
    "비록 길이에 의해 일반화를 하는것이 일반적이지만,\n",
    "\n",
    "$$\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n",
    "{\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)$$\n",
    "\n",
    "여기서 ϕ는 두 벡터 사이의 각도이다. 즉 엄청나게 비슷한 단어들은 1, 비슷하지 않은 단어들은 -1의 유사성을 가질 것이다.\n",
    "\n",
    "처음에 소개했던 sparse한 원-핫 벡터를 방금 정의했던 새로운 벡터의 특별한 경우로 생각할 수 있다. \n",
    "\n",
    "각 단어는 기본적으로 0의 유사성을 가지며, 우리는 각 단어에 대해 의미적인 속성을 부여한 것이다.\n",
    "\n",
    "이러한 새로운 벡터들은 dense하며, 0인 원소가 거의 없다고 말할 수 있다.\n",
    "\n",
    "하지만 이렇게 만들어진 새로운 벡터들은 고통 그 자체이다: 유사성을 측정하기 위해 수천개의 의미적인 속성을 생각해야 하며, 각기 다른 속성의 값을 어떻게 설정해야 할까?\n",
    "\n",
    "딥러닝의 핵심 아이디어는 neural netowrk가 프로그래머가 직접 설계하는 대신, 스스로  feature의 표현을 학습하는 것이다.\n",
    "\n",
    "word embedding이 그저 우리의 모델의 파라미터가 되어서 학습과정에서 업데이트 되도록 할 수는 없을까?\n",
    "\n",
    "이것이 우리가 앞으로 해야 할 일이다.\n",
    "\n",
    "원리대로라면, 우리는 netowrk가 잠재적으로 의미적인 속성을 가질 것이라고 생각한다.\n",
    "\n",
    "참고로 word embedding은 그 해석이 거의 불가능한 모델이다.\n",
    "\n",
    "비록 우리가 mathematician과 physicist가 둘 다 커피를 좋아하기 때문에 비슷하다고 생각하여 직접 만든 벡터들이 있다 할지라도, \n",
    "\n",
    "neural network가 학습한 embedding을 봤을 때,두 번째 차원에서 큰 값을 가지고 있다 해도 그 속성이 어떤 의미를 지니는지 알 길이 없는 것이다. \n",
    "\n",
    "어떤 잠재적인 의미적 차원 상에서는 비슷한 점이 있겠지만, 우리가 해석할 수 있는 것은 아닐 것이다.\n",
    "\n",
    "**요약하자면, word embedding은 단어의 *의미* 를 표현하는 것이고, 주어진 문제와 관련된 의미적인 정보를 효율적으로 encode 한다.** \n",
    "\n",
    "우리는 다른 것도 embed할 수 있다: 품사 정리, 구문 분석, 어떤것이든! Feature embedding은 NLP에서 핵심적인 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제를 실행하기 전에, 일반적인 딥러닝 프로그래밍 상황에서 embedding을 사용하는 것에 대해 몇 가지 참고 사항을 잠깐 얘기하고자 한다.\n",
    "\n",
    "원-핫 벡터를 만들기 위해 각 단어에 대해 고유 인덱스를 만들어줘야 하는 것처럼, 우리는 embedding을 사용할 때에도 고유 인덱스가 필요하다.\n",
    "\n",
    "이것이 테이블을 참조하는 키가 될 것이다. \n",
    "\n",
    "즉, embedding은 |V|×D matrix에 저장이 될 것인데, 여기서 D는 embedding의 차원이고, index i에 할당된 단어는 이 매트릭스의 i 번째 행에 저장되어진다.\n",
    "\n",
    "여기 제공되는 모든 코드에서 단어에서 index로 가는 mapping은 word_to_ix라는 사전이다.\n",
    "\n",
    "torch.nn.Embedding에서 제공하는 embedding은 두 가지 인자를 필요로 하는데, 사전의 크기와 embedding을 할 차원이다.\n",
    "\n",
    "테이블 내에서 인덱스를 참조하기 위해서는 반드시 torch.LongTensor를 사용해야 한다. Float 값은 인덱스로 쓰일 수가 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:59:39.163628Z",
     "start_time": "2019-11-17T13:59:38.885375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28c33fcc210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:59:39.170635Z",
     "start_time": "2019-11-17T13:59:39.164629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example: N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram 모델에서 우리는 단어 w들이 연속적으로 sequence형태로 w=(w0,w1,⋯)가 주어졌을 때 다음을 계산해내길 원한다.\n",
    "\n",
    "$$ P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} ) $$\n",
    "\n",
    "단어의 sequence에서 i번쨰 단어를 wi라고 합니다.\n",
    "\n",
    "아래 예제에서는, 우리는 손실함수를 계산하고 역전파를 통하여 파라미터들을 업데이트 할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:59:39.602031Z",
     "start_time": "2019-11-17T13:59:39.171635Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
      "[520.7720084190369, 518.1489133834839, 515.543007850647, 512.9515180587769, 510.374347448349, 507.81025290489197, 505.26042914390564, 502.72311568260193, 500.1970012187958, 497.6807861328125]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Computing Word Embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous bag-of-words (CBOW) 모델은 NLP deep learning에서 자주 사용되는 방법이다.\n",
    "\n",
    "모델은 문맥을 고려하여, target 단어 앞 뒤로 올 수 있을 몇개의 알맞을 단어를 예측한다.\n",
    "\n",
    "CBOW는 연속적이지 않고 굳이 확률적일 필요가 없기 때문에 다른 언어 모델들과 구별된다.\n",
    "\n",
    "Target 단어 wi와 양 쪽 문맥 범위가 N(window size)이어서 wi−1,⋯,wi−N과 wi+1,⋯,wi+N이 주어졌을 때 이 문맥 단어들의 모음을 C라고 한다.\n",
    "\n",
    "CBOW는 다음을 최소화하고자 한다.\n",
    "\n",
    "$$ -\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b) $$\n",
    "\n",
    "여기서 qw는 단어 w에 대한 embedding이다.\n",
    "\n",
    "아래의 class를 채워서 Pytorch로 모델을 실행해보자. 팁을 주자면:\n",
    "\n",
    "* 당신이 필요한 파라미터가 무엇무엇인지 생각해보자.\n",
    "\n",
    "* 각 연산이 필요로 하는 데이터의 모양이 무엇인지 확실하게 정리해라. 모양을 바꿔야 할 필요가 있다면 .view()를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T13:59:39.611039Z",
     "start_time": "2019-11-17T13:59:39.603032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([44, 29, 17, 31])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
