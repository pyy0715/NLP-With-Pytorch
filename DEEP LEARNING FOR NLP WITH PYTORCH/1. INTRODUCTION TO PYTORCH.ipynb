{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본: https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#sphx-glr-beginner-nlp-pytorch-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Torch’s tensor library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 계산은 모두 tensor를 통해 이루어진다. Tensor는 matrix를 일반화한 개념으로, 2차원 이상의 index를 가질 수 있다. \n",
    "\n",
    "나중에 tensor가 정확히 어떤 의미를 가지는지 알아볼 것이다.\n",
    "\n",
    "우선 우리가 tensor를 가지고 뭘 할 수 있는 지 한 번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T15:48:17.100397Z",
     "start_time": "2019-11-16T15:48:16.179663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25b8ed1c1d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Robert Guthrie\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor는 torch.Tensor() 함수가 Python list를 받아서 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T15:50:28.269604Z",
     "start_time": "2019-11-16T15:50:28.233440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D tensor란 무엇인가? 이렇게 생각해 보자.\n",
    "\n",
    "Vector의 원소 하나 하나는 스칼라이다.\n",
    "\n",
    "Matrix의 원소 하나 하나는 벡터다.\n",
    "\n",
    "그렇다면 3D tensor의 원소 하나 하나로는 matrix가 나오는 것이다!\n",
    "\n",
    "> **용어 정리**: 앞으로 이 튜토리얼에서의 \"tensor\"는 torch.Tensor object를 의미하는 용어로 사용할 것이다. \n",
    ">\n",
    "> Vector와 matrix도 torch.Tensor의 차원이 1, 2인 간단한 경우로 속한다. \n",
    ">\n",
    "> 차원을 명시할 필요가 있을 경우에는 꼭 밝힐 것이다. \n",
    ">\n",
    "> 예를 들어 3차원 tensor를 가지고 설명할 경우에는 그냥 \"tensor\"가 아닌 \"3D tensor\"라고 쓸 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T15:56:50.652938Z",
     "start_time": "2019-11-16T15:56:50.645930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 자료 유형에 대해서 텐서를 만들 수 있다. \n",
    "\n",
    "이미 위 코드에서 봤듯이, 기본 세팅은 Float이다. \n",
    "\n",
    "정수 형태의 tensor를 만들 때는 torch.LongTensor()를 사용하자. \n",
    "\n",
    "그 외 다른 경우를 위해서는 documentation을 확인해보길 바란다. \n",
    "\n",
    "하지만 대부분의 경우에 Float과 Long로 충분할 것이다.\n",
    "\n",
    "torch.randn()을 통해서 원하는 차원의 난수 tensor를 생성할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T16:08:29.201323Z",
     "start_time": "2019-11-16T16:08:29.192316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
      "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
      "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
      "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
      "\n",
      "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
      "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
      "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
      "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
      "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
      "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원하는 방식으로 Tensor간의 Operaiton이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T16:09:27.496616Z",
     "start_time": "2019-11-16T16:09:27.483737Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation을 보면 우리가 tensor를 통해 할 수 있는 수 많은 operation 리스트를 볼 수 있을 것이다. \n",
    "\n",
    "그 중에는 수학적인 연산의 개념을 넘어서는 operation도 있다.\n",
    "\n",
    "그 중에서 자주 사용하게 될 concatenation을 사용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T16:11:04.800671Z",
     "start_time": "2019-11-16T16:11:04.792361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n",
      "        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n",
      "        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n",
      "        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n",
      "        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\n",
      "tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,  1.2381],\n",
      "        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668, -0.9999]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".view() 방법을 이용하여 Tensor의 모양을 바꿔보자.\n",
    "\n",
    "이 방법은 대단히 많이 사용되는데, 이유로는 많은 neural network의 component들이 자기의 input이 어떤 특정 모양을 가져야 하기 때문이다.\n",
    "\n",
    "당신의 데이터를 component에게 보내기 전에 모양을 바꿔줄 필요가 종종 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T16:19:55.552817Z",
     "start_time": "2019-11-16T16:19:55.539289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n",
      "         [-0.6240, -0.9773,  0.8748,  0.9873],\n",
      "         [-0.0594, -2.4919,  0.2423,  0.2883]],\n",
      "\n",
      "        [[-0.1095,  0.3126,  1.5038,  0.5038],\n",
      "         [ 0.6223, -0.4481, -0.2856,  0.3880],\n",
      "         [-1.1435, -0.6512, -0.1032,  0.6937]]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
      "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512, -0.1032,  0.6937]])\n",
      "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
      "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
      "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
      "         -1.1435, -0.6512, -0.1032,  0.6937]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation graph는 효율적인 딥러닝 개발을 위해 필수적인 개념이다. \n",
    "\n",
    "왜냐하면 computation graph가 우리 대신에 back propagation gradient를 계산해주기 때문이다. \n",
    "\n",
    "Computation graph를 간단하게 말하자면, 어떻게 데이터가 결합돼서 output으로 계산이 된 것인지를 담은 기록장이다. \n",
    "\n",
    "어느 parameter가 어느 연산과 연계되었는지를 모두 기록하기 때문에 computation graph는 미분을 계산할 수 있을 정도로 충분한 정보를 갖게 된다. \n",
    "\n",
    "지금까지 설명한 것이 확실하게 와닿지 않을 수 있으므로, Pytorch의 핵심 class 중 하나인 autograd.Variable가 어떻게 작동하는 지 직접 보려고 한다.\n",
    "\n",
    "먼저 프로그래머의 관점에서 생각해보겠다. \n",
    "\n",
    "위에서 우리가 만든 torch.Tensor object에는 무엇이 담겨져 있을까? \n",
    "\n",
    "당연하게도 데이터가 있을 것이고, 그 모양(shape) 정보도 있을 것이고, 기타 등등이 있을 것이다. \n",
    "\n",
    "근데 두 tensor가 더해질 때를 생각해 보자. \n",
    "\n",
    "그 결과로 받는 tensor도 그 데이터와 모양 정도는 알고 있겠지만, 그 tensor 입장에서 자신이 다른 두 tensor의 합이라는 사실을 알고 있을 리가 없다 (마찬가지로 tensor는 자기가 파일에서 읽어서 생성된 tensor인지, 다른 operation으로 생성된 것인지 알 수 없다.)\n",
    "\n",
    "만약, requires_grad=True로 주게 되면 Tensor는 객체가 어떻게 생성되었는지 추적한다. 실제로 확인해보자! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T17:12:18.761426Z",
     "start_time": "2019-11-16T17:12:18.752417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000025B91137208>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag\n",
    "x = torch.tensor([1., 2., 3], requires_grad=True)\n",
    "\n",
    "# With requires_grad=True, you can still do all the operations you previously\n",
    "# could\n",
    "y = torch.tensor([4., 5., 6], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 Tensors는 무엇을 만들었는지 알고 있습니다. \n",
    "\n",
    "z는 파일에서 읽지 않았고 곱셈이나 지수 등의 결과가 아니라는 것을 알고 있습니다. \n",
    "\n",
    "그리고 z.grad_fn을 계속 따라가면 x와 y를 찾을 수 있습니다.\n",
    "\n",
    "하지만 그래디언트를 계산하는 데 어떻게 도움이됩니까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T17:26:26.946721Z",
     "start_time": "2019-11-16T17:26:26.931682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x0000025B91137D08>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 z의 총합 s를 x의 첫 번째 변수에 대한 미분값이 뭘까? 수식으로 간단하게 쓰자면 우리가 구하고 싶은 것은 이거다.\n",
    "\n",
    "$$\\frac{\\partial s}{\\partial x_0}$$\n",
    "\n",
    "s는 자기가 tensor z의 합으로 만들어졌다는 것을 안다. z는 x + y의 결과라는 것을 안다. 따라서\n",
    "\n",
    "$$s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}$$\n",
    "\n",
    "이고 s는 우리가 원하는 미분값이 1이라고 말해줄 수 있을만 한 충분한 정보를 다 갖고 있는 셈이다!\n",
    "\n",
    "물론 지금까지 설명한 것으로는 진짜로 미분을 어떻게 계산하는 지를 완벽하게 설명할 순 없다. \n",
    "\n",
    "요점은 s가 미분을 계산하기 위한 재료를 계속 가지고 다닌다는 것이다. \n",
    "\n",
    "실제로 Pytorch 개발자들은 sum()과 + 연산 자체가 스스로 gradient를 계산하는 방법을 알고, back propagation 알고리즘을 수행할 수 있도록 고안했다. \n",
    "\n",
    "더 자세한 내용은 이 튜토리얼의 범위를 벗어나므로 여기까지만 설명하겠다.\n",
    "\n",
    "이제 직접 Pytorch로 gradient를 계산하고 위에서 설명한 것과 맞는지 확인해보자. \n",
    "\n",
    "참고로, 만약 아래 cell을 여러 번 실행한다면 gradient는 누적 합산된다. \n",
    "\n",
    "이것은 Pytorch가 의도한 설계로, gradient를 .grad로 누적 시키는 것이 여러 모델에서 편리하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T17:32:29.589763Z",
     "start_time": "2019-11-16T17:32:29.524609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좋은 딥러닝 프로그래머가 되려면 아래 cell에서 어떤 일이 일어나는 지를 꼭 잘 이해해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T17:40:00.272057Z",
     "start_time": "2019-11-16T17:40:00.264050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n",
      "<AddBackward0 object at 0x0000025B91136588>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "# By default, user created Tensors have ``requires_grad=False``\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "# So you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "\n",
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "# flag in-place. The input flag defaults to ``True`` if not given.\n",
    "x = x.requires_grad_()\n",
    "y = y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(z.requires_grad)\n",
    "\n",
    "# Now z has the computation history that relates itself to x and y\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "new_z = z.detach()\n",
    "\n",
    "# ... does new_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_z.grad_fn)\n",
    "# And how could it? ``z.detach()`` returns a tensor that shares the same storage\n",
    "# as ``z``, but with the computation history forgotten. It doesn't know anything\n",
    "# about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.no_grad ()로 코드 블록을 감싸서, .requires_grad ''= True를 사용하여 Tensor에서 히스토리 추적을 중지 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T17:44:02.958496Z",
     "start_time": "2019-11-16T17:44:02.951490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
