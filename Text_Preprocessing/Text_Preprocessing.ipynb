{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyy0715/NLP-with-Deep-Learning/blob/master/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io4BeIPWd50g",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "### *자료출처: https://wikidocs.net/21698*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKwi6jPac5bv",
        "colab_type": "text"
      },
      "source": [
        "## 1.Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6ah6oalMwl",
        "colab_type": "text"
      },
      "source": [
        "### 토큰화 중 생기는 선택의 순간"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcMhVuDOcFDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptMg_prBcdJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "eeaee37a-2031-4d96-8177-c56907284981"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45qn821JcHA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "69595f6c-de8f-4fe3-a838-a15e0898462b"
      },
      "source": [
        "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQnz16DddACv",
        "colab_type": "text"
      },
      "source": [
        "* word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리한 것을 확인할 수 있습니다.\n",
        "그렇다면, wordPunctTokenizer는 아포스트로피가 들어간 코퍼스를 어떻게 처리할까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLTotrvdcHDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "77b1b386-32ff-4f85-b8ba-c8367fad27bc"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer  \n",
        "WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'be',\n",
              " 'fooled',\n",
              " 'by',\n",
              " 'the',\n",
              " 'dark',\n",
              " 'sounding',\n",
              " 'name',\n",
              " ',',\n",
              " 'Mr',\n",
              " '.',\n",
              " 'Jone',\n",
              " \"'\",\n",
              " 's',\n",
              " 'Orphanage',\n",
              " 'is',\n",
              " 'as',\n",
              " 'cheery',\n",
              " 'as',\n",
              " 'cheery',\n",
              " 'goes',\n",
              " 'for',\n",
              " 'a',\n",
              " 'pastry',\n",
              " 'shop',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMhi1k8TdMEw",
        "colab_type": "text"
      },
      "source": [
        "* WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리하였으며, 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리한 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z_LIY-tlXCE",
        "colab_type": "text"
      },
      "source": [
        "### 토큰화에서 고려해야할 사항"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoA_oWTLcHGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "df4c12da-1e92-4851-8b50-abc12cc4da11"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Starting',\n",
              " 'a',\n",
              " 'home-based',\n",
              " 'restaurant',\n",
              " 'may',\n",
              " 'be',\n",
              " 'an',\n",
              " 'ideal.',\n",
              " 'it',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'have',\n",
              " 'a',\n",
              " 'food',\n",
              " 'chain',\n",
              " 'or',\n",
              " 'restaurant',\n",
              " 'of',\n",
              " 'their',\n",
              " 'own',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7aBPKQod1US",
        "colab_type": "text"
      },
      "source": [
        "* Penn Treebank Tokenization의 규칙에 대해서 소개하고, Tokenization의 결과를 보도록 하겠습니다.\n",
        "\n",
        "> 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
        "\n",
        "\n",
        "> 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un21sNOQlcNt",
        "colab_type": "text"
      },
      "source": [
        "### 문장 토큰화(Sentence Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvVSfCidOqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b2c82f0b-324f-4496-89bf-a468bd895733"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxF_Cx6wdOsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff73e71d-0240-47fa-9905-57b46e6a7b3a"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nzxk_HZj3co",
        "colab_type": "text"
      },
      "source": [
        "* NLTK는 단순히 온점을 구분자로 하여 문장을 구분하지 않았기 때문에, Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQiCq_delDsF",
        "colab_type": "text"
      },
      "source": [
        "### 한국어에서의 토큰화의 어려움."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk7YdH_6ki44",
        "colab_type": "text"
      },
      "source": [
        " 한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야 합니다. 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말합니다. 이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.\n",
        "\n",
        "자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\n",
        "\n",
        "의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다.\n",
        "\n",
        "* 예를 들어 다음과 같은 문장이 있다고 합시다.\n",
        "\n",
        "문장 : 에디가 딥러닝책을 읽었다\n",
        "이를 형태소 단위로 분해하면 다음과 같습니다.\n",
        "\n",
        "자립 형태소 : 에디, 딥러닝책\n",
        "\n",
        "의존 형태소 : -가, -을, 읽-, -었, -다\n",
        "\n",
        "이를 통해 유추할 수 있는 것은 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 형태소 토큰화를 수행해야한다는 겁니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM8ll1TBlk1k",
        "colab_type": "text"
      },
      "source": [
        "### NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUurYWHblpBX",
        "colab_type": "text"
      },
      "source": [
        "* NLTK에서는 영어 코퍼스에 품사 부착 기능을 지원하고 있습니다. 품사를 어떻게 명명하고, 부착하는지의 기준은 여러가지가 있는데, NLTK에서는 Penn Treebank POS Tags라는 기준을 사용합니다. 실제로 NLTK를 사용해서 영어 코퍼스에 품사 부착을 해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUDeO7Y4l0n-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5603180c-a96c-4967-97fc-1b51f1e875ea"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06qa0qHedOu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f786783d-9afe-47fe-eca4-1f947e3eb4ad"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7pMIjNSdOxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "9bc4d7f4-0a26-4630-989c-53d81945676d"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "x=word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gclu7BW0l6Nm",
        "colab_type": "text"
      },
      "source": [
        "* Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UM0DSIqcHIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "6c24ccf1-e9f4-43b3-fdd1-e2c641711c94"
      },
      "source": [
        "# pip install konlpy"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 3.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 60.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNq_xF95mk0P",
        "colab_type": "text"
      },
      "source": [
        "* 한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행하게 됨을 뜻합니다. 여기선 이 중에서 Okt와 꼬꼬마를 통해서 토큰화를 수행해보도록 하겠습니다. (Okt는 기존에는 Twitter라는 이름을 갖고있었으나 0.5.0 버전부터 이름이 변경되어 인터넷에는 아직 Twitter로 많이 알려져있으므로 학습 시 참고바랍니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m_IDNGzdO0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c9a276d-1a72-4bef-8770-3b0c033069ca"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgbnNZ7JcHKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e9a314f9-bd22-402f-f82d-d03bf06056d2"
      },
      "source": [
        "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLF5mAFqcHM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e3ae647-f412-4c58-9a28-2d1f0b78f5db"
      },
      "source": [
        "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhyie7ZCm47-",
        "colab_type": "text"
      },
      "source": [
        "* 위의 예제는 Okt 형태소 분석기로 토큰화를 시도해본 예제입니다.\n",
        "\n",
        "\n",
        "> 1) morphs : 형태소 추출\n",
        "\n",
        "> 2) pos : 품사 부착(Part-of-speech tagging)\n",
        "\n",
        "> 3) nouns : 명사 추출\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McoOUMz-nVYD",
        "colab_type": "text"
      },
      "source": [
        "* 이번에는 꼬꼬마 형태소 분석기를 사용하여 같은 문장에 대해서 토큰화를 진행해볼 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us1cbwoFmfu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1570a0b5-2dc8-46e3-deca-704655e21dc5"
      },
      "source": [
        "from konlpy.tag import Kkma  \n",
        "kkma=Kkma()  \n",
        "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixejOmV8mfr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a45a8edf-a8b8-4f47-dd60-693afcc641e6"
      },
      "source": [
        "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5R11islcHPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebbd095e-b5af-4591-9d46-d8fe7bb058a2"
      },
      "source": [
        "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['코딩', '당신', '연휴', '여행']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SruN7CRHnjUB",
        "colab_type": "text"
      },
      "source": [
        "* 앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다. 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. 예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGg5bYonnptk",
        "colab_type": "text"
      },
      "source": [
        "## 2.정제(Cleaning) and 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIr6_zHcn49N",
        "colab_type": "text"
      },
      "source": [
        "코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(Tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(Cleaning) 및 정규화(Normalization)하는 일이 항상 함께합니다. 정제 및 정규화의 목적은 각각 다음과 같습니다.\n",
        "\n",
        "* 정제(Cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "\n",
        "* 정규화(Normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
        "정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 합니다. 사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾기도 합니다.\n",
        "\n",
        "이 챕터에서는 여러가지 정제와 정규화 기법에 대해서 배웁니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMoB4kF-pHoB",
        "colab_type": "text"
      },
      "source": [
        "### 불필요한 단어의 제거(Removing Unnecessary Words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTrcbwOPnkHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c20b513-b99c-4778-fb39-63300cebf630"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU5becXyr-NG",
        "colab_type": "text"
      },
      "source": [
        "## 3.어간 추출(Stemming) and 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoaxODbtsMyi",
        "colab_type": "text"
      },
      "source": [
        "### 표제어 추출(Lemmatization)\n",
        "\n",
        "표제어 추출은 단어들이 서로 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지를 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이 때, 이 단어들의 Lemma는 be라고 합니다.\n",
        "\n",
        "표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행합니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(Morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻합니다.\n",
        "\n",
        "형태소는 두 가지 종류가 있습니다. 각각 stem(어간)과 affix(접사)입니다.\n",
        "* 1) stem(어간)\n",
        ": 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
        "\n",
        "* 2) affix(접사)\n",
        ": 단어에 추가적인 의미를 주는 부분.\n",
        "\n",
        "형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말합니다. 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리합니다. 물론, 꼭 두 가지로 분리되지 않는 경우도 있습니다. 가령, 단어 fox는 형태학적 파싱을 한다고 하더라도 더 이상 분리할 수 없습니다. fox는 독립적인 형태소이기 때문입니다. 이와 유사하게, cat 또한 더 이상 분리되지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaR0z4LgssJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "57366bd7-9a9c-4239-ae2e-e52244424a0f"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPXT2Fb_ntXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "19287bb2-2f5f-433a-bc18-7ea95959a6e4"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "[n.lemmatize(w) for w in words]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['policy',\n",
              " 'doing',\n",
              " 'organization',\n",
              " 'have',\n",
              " 'going',\n",
              " 'love',\n",
              " 'life',\n",
              " 'fly',\n",
              " 'dy',\n",
              " 'watched',\n",
              " 'ha',\n",
              " 'starting']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_hoD2r6s_EU",
        "colab_type": "text"
      },
      "source": [
        "*  표제어 추출은 어간 추출(Stemming)과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다. 하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. 이는 표제어 추출기(Lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk4vhbYkntVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05c2dfe3-f5ad-4e93-f559-ea0833a5f8ff"
      },
      "source": [
        "n.lemmatize('dies', 'v')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi_A2zi0tB9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a741dcf-b4cd-42a4-ad12-cc44ae1cfcd9"
      },
      "source": [
        "n.lemmatize('watched', 'v')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXLBXAsdtCEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3433054-2246-45c2-c744-03936561ee00"
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdSKpEKMtvGF",
        "colab_type": "text"
      },
      "source": [
        "Stemming(어간 추출)에 대해서 언급하기에 앞서, Lemmatization(표제어 추출)과 Stemming(어간 추출)의 차이에 대해 미리 언급하고자 합니다. 표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. (POS 태그를 보존한다고도 말할 수 있습니다.)\n",
        "\n",
        "하지만, 어간 추출을 수행한 결과는 품사 정보가 보존되지 않습니다. (다시 말해 POS 태그를 고려하지 않습니다.) 더 정확히는, 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTqj7tPKtxpI",
        "colab_type": "text"
      },
      "source": [
        "### 어간 추출(Stemming)\n",
        "\n",
        "Stem(어간)을 추출하는 작업을 Stemming이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXQm3N5MtIY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f9d0e172-efe8-473f-9e83-4e14c40949a2"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EexTLOWt1RJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "0bfbb675-3e3c-4675-87cd-b62482eeb4e4"
      },
      "source": [
        "[s.stem(w) for w in words]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thi',\n",
              " 'wa',\n",
              " 'not',\n",
              " 'the',\n",
              " 'map',\n",
              " 'we',\n",
              " 'found',\n",
              " 'in',\n",
              " 'billi',\n",
              " 'bone',\n",
              " \"'s\",\n",
              " 'chest',\n",
              " ',',\n",
              " 'but',\n",
              " 'an',\n",
              " 'accur',\n",
              " 'copi',\n",
              " ',',\n",
              " 'complet',\n",
              " 'in',\n",
              " 'all',\n",
              " 'thing',\n",
              " '--',\n",
              " 'name',\n",
              " 'and',\n",
              " 'height',\n",
              " 'and',\n",
              " 'sound',\n",
              " '--',\n",
              " 'with',\n",
              " 'the',\n",
              " 'singl',\n",
              " 'except',\n",
              " 'of',\n",
              " 'the',\n",
              " 'red',\n",
              " 'cross',\n",
              " 'and',\n",
              " 'the',\n",
              " 'written',\n",
              " 'note',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw4JLLZeu1Wu",
        "colab_type": "text"
      },
      "source": [
        "가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.\n",
        "\n",
        "ALIZE → AL\n",
        "\n",
        "ANCE → 제거\n",
        "\n",
        "ICAL → IC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czb0Z1Ydt1UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71e9e8f1-7b10-410e-ab69-1007cd62d632"
      },
      "source": [
        "s = PorterStemmer()\n",
        "words=['formalize', 'allowance', 'electricical']\n",
        "[s.stem(w) for w in words]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['formal', 'allow', 'electric']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0z_r-uvvFVd",
        "colab_type": "text"
      },
      "source": [
        "어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다.\n",
        "\n",
        "NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다. 이번에는 포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때, 이 둘의 결과를 비교해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7YqDBtlt1O9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "540e36ca-4be2-4838-e8cf-a823803d4364"
      },
      "source": [
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "[s.stem(w) for w in words]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['polici',\n",
              " 'do',\n",
              " 'organ',\n",
              " 'have',\n",
              " 'go',\n",
              " 'love',\n",
              " 'live',\n",
              " 'fli',\n",
              " 'die',\n",
              " 'watch',\n",
              " 'ha',\n",
              " 'start']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr-D4EDlvHR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "f16e2102-3397-4ac1-c494-1205d51cf13c"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "[l.stem(w) for w in words]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['policy',\n",
              " 'doing',\n",
              " 'org',\n",
              " 'hav',\n",
              " 'going',\n",
              " 'lov',\n",
              " 'liv',\n",
              " 'fly',\n",
              " 'die',\n",
              " 'watch',\n",
              " 'has',\n",
              " 'start']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y7aLtlhvXuV",
        "colab_type": "text"
      },
      "source": [
        "동일한 단어들의 나열에 대해서 두 스태머는 전혀 다른 결과를 보여줍니다. 두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문입니다. 그렇기 때문에 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용하여야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05kX9O8hv7gt",
        "colab_type": "text"
      },
      "source": [
        "## 4.불용어(Stopword)\n",
        "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 문장 내에서는 자주 등장하지만 문장을 분석하는 데 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다. 그렇기 때문에 NLTK를 사용하면, 갖고 있는 코퍼스로부터 NLTK에서 이미 정의된 영어 불용어들을 쉽게 제거할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QW2ZkjUwJ8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6ead3046-c2e9-4a96-9bd5-8a32d069d441"
      },
      "source": [
        " nltk.download('stopwords')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70x78MnuvHVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07265c50-9755-4e37-d63a-96969a795c4d"
      },
      "source": [
        "from nltk.corpus import stopwords  \n",
        "stopwords.words('english')[:10]  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G00slAQlwILX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "610c3d45-9bda-483a-df26-169c0ff82ca5"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)   \n",
        "result = [] \n",
        "\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm5C1Lhlxz4N",
        "colab_type": "text"
      },
      "source": [
        "### 한국어에서 불용어 제거하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXyApHbcxrLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "881ad641-cf79-41d2-ba64-8af6a4df3d07"
      },
      "source": [
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words=stop_words.split(' ')\n",
        "\n",
        "word_tokens = word_tokenize(example)   \n",
        "result = [] \n",
        "\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(word_tokens) \n",
        "print(result)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p827y79yQt4",
        "colab_type": "text"
      },
      "source": [
        "## 5.정규표현식\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbkUlgJy76v",
        "colab_type": "text"
      },
      "source": [
        "* https://wikidocs.net/21703"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Qh9ouZzWpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2c5f3a06-d993-4410-f052-b9d2181f9711"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HGI97kt2quq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9e50c233-3f8d-424a-9665-8475f4c6f17e"
      },
      "source": [
        "tokenizer=RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1eLH0w3FYU",
        "colab_type": "text"
      },
      "source": [
        "## 6.단어 분리(Subword Segmentation)\n",
        "단어 분리(Subword segmenation) 작업은 하나의 단어는 의미있는 여러 단어들의 조합으로 구성된 경우가 많기 때문에, 단어를 여러 단어로 분리해보겠다는 전처리 작업입니다. 실제로, 언어의 특성에 따라 영어권 언어나 한국어는 단어 분리를 시도했을 때 어느정도 의미있는 단위로 나누는 것이 가능합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_y0nD1x3xUF",
        "colab_type": "text"
      },
      "source": [
        "### WPM(Word Piece Model)\n",
        "WPM(Word Piece Model)은 하나의 단어를 내부 단어(Subword Unit)들로 분리하는 단어 분리 모델입니다.\n",
        "\n",
        "실제로 2016년에 구글이 낸 논문(Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [Wo at el.2016])에서 구글은 자신들의 구글 번역기에서 WPM이 어떻게 수행되는지에 대해서 기술하였습니다.\n",
        "\n",
        "* WPM을 수행하기 이전의 문장: Jet makers feud over seat width with big orders at stake\n",
        "* WPM을 수행한 결과(wordpieces): _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\n",
        "\n",
        "Jet는 J와 et로 나누어졌으며, feud는 fe와 ud로 나누어진 것을 볼 수 있습니다. WPM은 입력 문장에서 기존에 존재하던 띄어쓰기는 언더바로 치환하고, 단어는 내부단어(subword)로 통계에 기반하여 띄어쓰기로 분리합니다. 기존의 띄어쓰기를 언더바로 치환하는 이유는 차후 다시 문장 복원을 위한 장치입니다. WPM의 결과로 나온 문장을 보면, 기존에 없던 띄어쓰기가 추가되어 내부 단어(subwords)들을 구분하는 구분자 역할을 하고 있으므로 본래의 띄어쓰기를 언더바로 치환해놓지 않으면, 기존 문장으로 복원할 수가 없습니다. WPM이 수행된 결과로부터 다시 수행 전의 결과로 돌리는 것 방법은 현재 있는 띄어쓰기를 전부 삭제하여 내부 단어들을 다시 하나의 단어로 연결시키고, 언더바를 다시 띄어쓰기로 바꾸면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs8fjeOu4F7B",
        "colab_type": "text"
      },
      "source": [
        "### BPE(Byte Pair Encoding) 알고리즘\n",
        "구글의 WPM에는 BPE(Byte Pair Encoding) 알고리즘이 사용되었습니다. BPE 알고리즘 자체는 1994년에 제안된 알고리즘이지만 Neural Machine Translation of Rare Words with Subword Units [Sennrich at el.2015]라는 논문에서 최초로 기계 번역을 위해 BPE 알고리즘을 통해 단어 분리(Subword Segmentation)를 사용할 것 제안하였습니다. 그리고 현재는 자연어 처리를 위한 주요 전처리 방법으로 사용되고 있습니다.\n",
        "\n",
        "아래의 코드는 해당 논문에서 예제로서 사용된 BPE 알고리즘의 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxIm3Sim26NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "b39b9648-102e-4e00-dc91-df38ab43ae1a"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out\n",
        "\n",
        "vocab = {'l o w </w>' : 5,\n",
        "         'l o w e r </w>' : 2,\n",
        "         'n e w e s t </w>':6,\n",
        "         'w i d e s t </w>':3\n",
        "         }\n",
        "\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = get_stats(vocab)\n",
        "    best = max(pairs, key=pairs.get)\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(best)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('e', 's')\n",
            "('es', 't')\n",
            "('est', '</w>')\n",
            "('l', 'o')\n",
            "('lo', 'w')\n",
            "('n', 'e')\n",
            "('ne', 'w')\n",
            "('new', 'est</w>')\n",
            "('low', '</w>')\n",
            "('w', 'i')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJa4OIUI7PZe",
        "colab_type": "text"
      },
      "source": [
        "## 7.데이터의 분리(Splitting Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPgvWe2B7yzw",
        "colab_type": "text"
      },
      "source": [
        "### 지도 학습(Supervised Learning)\n",
        "지도 학습의 훈련 데이터는 문제지를 연상케 합니다. 지도 학습의 훈련 데이터는 정답이 무엇인지 맞춰 하는 '문제'에 해당되는 데이터와 레이블이라고 부르는 '정답'이 적혀있는 데이터로 구성되어 있습니다. 쉽게 비유하면, 기계는 정답이 적혀져 있는 문제지를 문제와 정답을 함께 보면서 열심히 공부하고, 향후에 정답이 없는 문제에 대해서도 정답을 예측해서 대답하게 되는 메커니즘입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qnFUd0M75tn",
        "colab_type": "text"
      },
      "source": [
        "### X와 y분리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NesWB9bh3JAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "445f5b8c-28af-415a-e52b-ec5093177f16"
      },
      "source": [
        "X,y = zip(['a', 1], ['b', 2], ['c', 3])\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a', 'b', 'c')\n",
            "(1, 2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHKt1Jtd3JGC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5b7c931e-76df-483a-dd2b-f75c6416efd6"
      },
      "source": [
        "sequences=[['a', 1], ['b', 2], ['c', 3]] # 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.\n",
        "X,y = zip(*sequences) # *를 추가\n",
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a', 'b', 'c')\n",
            "(1, 2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "misax6-53JJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "eb236749-c1da-422d-d7f5-35491ec0d330"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "values = [['당신에게 드리는 마지막 혜택!', 1],\n",
        "['내일 뵐 수 있을지 확인 부탁드...', 0],\n",
        "['도연씨. 잘 지내시죠? 오랜만입...', 0],\n",
        "['(광고) AI로 주가를 예측할 수 있다!', 1]]\n",
        "columns = ['메일 본문', '스팸 메일 유무']\n",
        "\n",
        "df = pd.DataFrame(values, columns=columns)\n",
        "df"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>메일 본문</th>\n",
              "      <th>스팸 메일 유무</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>당신에게 드리는 마지막 혜택!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>내일 뵐 수 있을지 확인 부탁드...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>도연씨. 잘 지내시죠? 오랜만입...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(광고) AI로 주가를 예측할 수 있다!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    메일 본문  스팸 메일 유무\n",
              "0        당신에게 드리는 마지막 혜택!         1\n",
              "1    내일 뵐 수 있을지 확인 부탁드...         0\n",
              "2    도연씨. 잘 지내시죠? 오랜만입...         0\n",
              "3  (광고) AI로 주가를 예측할 수 있다!         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ds7YeZQ3JMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=df['메일 본문']\n",
        "y=df['스팸 메일 유무']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8MU3tIM3JDu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "34a42507-2c0e-4c5c-fec0-45280404cfd3"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0          당신에게 드리는 마지막 혜택!\n",
            "1      내일 뵐 수 있을지 확인 부탁드...\n",
            "2      도연씨. 잘 지내시죠? 오랜만입...\n",
            "3    (광고) AI로 주가를 예측할 수 있다!\n",
            "Name: 메일 본문, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EK2KHkh8MEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "65663add-4405-4184-d05c-1af26fdb1879"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    0\n",
            "2    0\n",
            "3    1\n",
            "Name: 스팸 메일 유무, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxQqXLdo8Nm9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5fdb97b8-dfce-40f2-c100-55663c0dc208"
      },
      "source": [
        "import numpy as np\n",
        "ar = np.arange(0,16).reshape((4,4))\n",
        "print(ar)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUlvKFyJ8QxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9d8ffdfa-661b-4325-e844-3b2399b6b238"
      },
      "source": [
        "X=ar[:, :3]\n",
        "print(X)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 4  5  6]\n",
            " [ 8  9 10]\n",
            " [12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o6u6NzV8SRu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa6529e7-cdb7-45d9-fe44-f60db38bad3d"
      },
      "source": [
        "y=ar[:,3]\n",
        "print(y)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3  7 11 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RZbzxzr8U4h",
        "colab_type": "text"
      },
      "source": [
        "### 테스트 데이터 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHVsjACM8TcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2tPHEbP8aBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8871d2b1-f601-466a-89fa-0b21a2a4ad0a"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = np.arange(10).reshape((5, 2)), range(5)\n",
        "# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성\n",
        "print(X)\n",
        "print(list(y)) #레이블 데이터"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1]\n",
            " [2 3]\n",
            " [4 5]\n",
            " [6 7]\n",
            " [8 9]]\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80mP--YI8cju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN92fH9P8kqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "63b4c176-0e44-4800-baea-51c9eb357198"
      },
      "source": [
        "print(X_train)\n",
        "print(X_test)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 3]\n",
            " [4 5]\n",
            " [6 7]]\n",
            "[[8 9]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMBTiwr-8ntH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3464c70a-48de-41de-d6c5-bea56bab64bd"
      },
      "source": [
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3]\n",
            "[4, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKwbutH_88z5",
        "colab_type": "text"
      },
      "source": [
        "# 8.정수 인코딩(Integer Encoding)\n",
        "컴퓨터는 텍스트보다는 숫자를 더 잘 처리 할 수 있습니다. 이를 위해 자연어 처리에서는 텍스트를 숫자로 바꾸는 여러가지 기법들이 있습니다. 그리고 그러한 기법들을 본격적으로 적용시키기 위한 첫 단계로 각 단어를 고유한 숫자에 맵핑(Mapping)시키는 전처리 작업이 필요할 때가 있습니다.\n",
        "\n",
        "예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 0번부터 4,999번까지 단어와 맵핑되는 고유한 숫자, 다른 말로는 인덱스를 부여합니다. (1번부터 시작해서 부여해도 상관은 없으며 코딩을 하는 동안 이를 인지하고 있으면 됩니다.) 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자가 부여됩니다. 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 전처리도 같이 겸하기 위해 단어에 대한 빈도수로 정렬한 뒤에 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4gcCEdn8qwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountai\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-1sxOA29JAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "99edf732-0724-4e8f-9a54-6daac5c23226"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=sent_tokenize(text)\n",
        "print(text)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountai']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGv4i4s9PmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0c2bae9a-ec95-459f-c065-de6fc9a530c2"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "vocab=Counter() # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.  \n",
        "\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for i in text:\n",
        "    sentence=word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
        "    result = []\n",
        "\n",
        "    for word in sentence: \n",
        "        word=word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                result.append(word)\n",
        "                vocab[word]=vocab[word]+1 #각 단어의 빈도를 Count 합니다.\n",
        "    sentences.append(result) \n",
        "print(sentences)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountai']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB1cIqJs9hD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8853f683-383b-4d76-9ca7-5d5e136ca3a4"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountai': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba-G6g_V9piR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "511e8868-f25b-4f06-bcb2-9aa7524a0442"
      },
      "source": [
        "vocab_sorted=sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountai', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7sF7VKU9yRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e69fc748-c9f2-491c-e1ec-b9f63da788db"
      },
      "source": [
        "word_to_index={}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
        "        i=i+1\n",
        "        word_to_index[word]=i\n",
        "print(word_to_index)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwXCNr-U-JkG",
        "colab_type": "text"
      },
      "source": [
        "### 케라스(Keras)의 텍스트 전처리\n",
        "케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공합니다. 때로는 정수 인코딩을 위해서 케라스의 전처리 도구인 토크나이저를 사용하기도 하는데, 사용 방법과 그 한계점에 대해서 이해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-BtlKeG96-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73fa4fd3-a39c-4dcc-dd74-127cc707a750"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "text=[\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"]\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgDM86xq-awx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "121a0929-6118-45fb-d5f3-b07b62f5f73c"
      },
      "source": [
        "print(t.word_index)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'his': 5, 'is': 6, 'kept': 7, 'person': 8, 'the': 9, 'he': 10, 'word': 11, 'keeping': 12, 'good': 13, 'knew': 14, 'but': 15, 'and': 16, 'such': 17, 'to': 18, 'himself': 19, 'was': 20, 'driving': 21, 'crazy': 22, 'went': 23, 'up': 24, 'mountain': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieOQ44oE-h4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "386f7546-2340-4299-e863-442e560dcd5e"
      },
      "source": [
        "print(t.word_counts)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('a', 8), ('barber', 8), ('is', 4), ('person', 3), ('good', 1), ('huge', 5), ('he', 2), ('knew', 1), ('secret', 6), ('the', 3), ('kept', 4), ('his', 5), ('word', 2), ('but', 1), ('keeping', 2), ('and', 1), ('such', 1), ('to', 1), ('himself', 1), ('was', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('up', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFF-fHnr-kYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7bd5e9bc-92df-4412-a337-f4979de1a90e"
      },
      "source": [
        "print(t.texts_to_sequences(text))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 6, 1, 8, 1, 2, 6, 13, 8, 1, 2, 6, 4, 8, 10, 14, 1, 3, 9, 3, 10, 7, 6, 4, 3, 4, 3, 5, 2, 7, 5, 11, 1, 2, 7, 5, 11, 5, 2, 7, 5, 3, 15, 12, 16, 12, 17, 1, 4, 3, 18, 19, 20, 21, 9, 2, 22, 9, 2, 23, 24, 1, 4, 25]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HmzldY8-ySm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9fbc0a55-1dca-4ec5-af10-f0f251e1ca3e"
      },
      "source": [
        "words_frequency = [w for w,c in t.word_counts.items() if c < 2] # 빈도수가 2미만 단어를 w라고 저장\n",
        "for w in words_frequency:\n",
        "    del t.word_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del t.word_counts[w] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "print(t.texts_to_sequences(text))\n",
        "print(t.word_index)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 6, 1, 8, 1, 2, 6, 8, 1, 2, 6, 4, 8, 10, 1, 3, 9, 3, 10, 7, 6, 4, 3, 4, 3, 5, 2, 7, 5, 11, 1, 2, 7, 5, 11, 5, 2, 7, 5, 3, 12, 12, 1, 4, 3, 9, 2, 9, 2, 1, 4]]\n",
            "{'a': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'his': 5, 'is': 6, 'kept': 7, 'person': 8, 'the': 9, 'he': 10, 'word': 11, 'keeping': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKWQ5sXw_gzC",
        "colab_type": "text"
      },
      "source": [
        "### enumerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE9GhJrz_WGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "e97c840a-ff9d-406b-b513-52cedf2da784"
      },
      "source": [
        "test=[8, 2, 5, 1, 3, 7, 9, 4, 6, 10]\n",
        "\n",
        "for index, value in enumerate(test): # 입력의 순서대로 0부터 인덱스를 부여함.\n",
        "  print(\"index : {}, value: {}\".format(index,value))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index : 0, value: 8\n",
            "index : 1, value: 2\n",
            "index : 2, value: 5\n",
            "index : 3, value: 1\n",
            "index : 4, value: 3\n",
            "index : 5, value: 7\n",
            "index : 6, value: 9\n",
            "index : 7, value: 4\n",
            "index : 8, value: 6\n",
            "index : 9, value: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RaUPwND_nlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문장 토큰화까지는 되어 있다고 가정함\n",
        "text=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynaVzlExADOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "83a221da-87d6-4ef8-96a5-e21a67fbb411"
      },
      "source": [
        "vocab=sum(text, [])\n",
        "print(vocab)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nlPcBPXAOJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "408953b9-c284-456f-81f1-57bdc09bac5f"
      },
      "source": [
        "vocab_sorted=(sorted(set(vocab)))\n",
        "print(vocab_sorted)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'crazy', 'driving', 'good', 'huge', 'keeping', 'kept', 'knew', 'mountain', 'person', 'secret', 'went', 'word']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6_R_UDGAT9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3a3c2d7b-9bbd-435b-bf0c-a6a2534151d9"
      },
      "source": [
        "word_to_index={word : index+1 for index, word in enumerate(vocab_sorted)}\n",
        "# 인덱스를 0이 아닌 1부터 부여.\n",
        "print(word_to_index)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'crazy': 2, 'driving': 3, 'good': 4, 'huge': 5, 'keeping': 6, 'kept': 7, 'knew': 8, 'mountain': 9, 'person': 10, 'secret': 11, 'went': 12, 'word': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuGpkhTVAcNI",
        "colab_type": "text"
      },
      "source": [
        "### NLTK의 FreqDist 클래스"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0jWQOlYAYaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_list = ['barber', 'barber', 'person', 'barber', 'good', 'person']\n",
        "from nltk import FreqDist\n",
        "fdist = FreqDist(test_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6sbnalAhom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ee0bf51-572e-4ba0-955e-2f7198ed2f52"
      },
      "source": [
        "fdist.N() # 전체 단어 개수 출력"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7gOa1tZArZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbd9dd48-a15d-49e0-ad28-a6a52319e072"
      },
      "source": [
        "fdist.freq(\"barber\") # 'barber'라는 단어의 확률."
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BChAYPQtAs8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d106b008-3a91-43f9-f024-adbbf5200119"
      },
      "source": [
        "fdist[\"barber\"] # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ROmKOblAwqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7aa91110-42bf-4d1e-9104-579808500956"
      },
      "source": [
        "fdist.most_common(2) # 등장 빈도수가 높은 상위 2개의 단어만 출력"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 3), ('person', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lavF3NUwA1Tc",
        "colab_type": "text"
      },
      "source": [
        "## 9.원-핫 인코딩(One-hot encoding)\n",
        "원-핫 인코딩에 대해서 배우기에 앞서 단어 집합(Vocabulary)에 대해서 정의해보도록 하겠습니다. 단어 집합은 앞으로 자연어 처리에서 계속 나오는 개념이기 때문에 여기서 이해하고 가야합니다. 단어 집합은 서로 다른 단어들의 집합입니다. 여기서 혼동이 없도록 서로 다른 단어라는 정의에 대해서 좀 더 주목할 필요가 있습니다.\n",
        "\n",
        "단어 집합(Vocabulary)에서는 기본적으로 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주합니다. 이 책에서는 앞으로 단어 집합에 있는 단어들을 가지고, 문자를 숫자(더 구체적으로는 벡터)로 바꾸는 원-핫 인코딩을 포함한 여러 방법에 대해서 배우게 됩니다.\n",
        "\n",
        "원-핫 인코딩을 위해서 먼저 해야할 일은 갖고 있는 우선 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허락하지 않고 모아놓는다면 이를 단어 집합이라고 합니다. 그리고 이 단어 집합에 고유한 숫자를 부여하는 일을 진행합니다. 갖고 있는 텍스트에 단어가 5,000개가 등장했다면, 단어 집합의 크기를 5,000이라고 합니다. 5,000개의 단어가 있는 이 단어 집합의 단어들마다 0번부터 4,999번까지 인덱스를 부여한다고 해보겠습니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 부여됩니다. (정수 인코딩 챕터 참고)\n",
        "\n",
        "이제 각 단어에 고유한 인덱스를 붙였다고 합시다. 이 숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 하면 될까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0uFPLEmAyzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7f270f87-31b6-430d-89ca-aa232b4b2156"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "token=okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(token)   \n",
        "['나', '는', '자연어', '처리', '를', '배운다']  "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['나', '는', '자연어', '처리', '를', '배운다']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk2kbHKFBPO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aea9bdb7-c0be-4884-d032-b7a424f74579"
      },
      "source": [
        "word2index={}\n",
        "for voca in token:\n",
        "     if voca not in word2index.keys():\n",
        "       word2index[voca]=len(word2index)\n",
        "print(word2index)\n",
        "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}  "
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'나': 0, '는': 1, '를': 4, '배운다': 5, '자연어': 2, '처리': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA7U1qNxBgej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(word, word2index):\n",
        "       one_hot_vector = [0]*(len(word2index))\n",
        "       index=word2index[word]\n",
        "       one_hot_vector[index]=1\n",
        "       return one_hot_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IishSaOqBkKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4163f33-0fe9-41c8-8045-126f4ea47d99"
      },
      "source": [
        "one_hot_encoding(\"자연어\",word2index)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cESjzHKkBuDY",
        "colab_type": "text"
      },
      "source": [
        "### 케라스(Keras)를 이용한 원-핫 인코딩(One-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN40VnzlBsPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxJWOL0wBxmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bad29276-b7c7-4932-885e-81444f1f8048"
      },
      "source": [
        "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "# 입력으로 [text]가 아닌 text를 넣을 경우 한 글자 단위 인코딩이 되버립니다. ex 갈 : 1, 래 : 2\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFslsswrB9Bh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10579af7-2e47-47c9-fc78-6470e08f193f"
      },
      "source": [
        "text2=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "x=t.texts_to_sequences([text2])\n",
        "print(x)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 5, 1, 6, 3, 7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDm--aL6CB3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4fa4d20-b2d1-4085-cfa9-fa55157c620c"
      },
      "source": [
        "text2=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "x=t.texts_to_sequences([text2])[0]\n",
        "print(x)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9-CGlvACGdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0ec33946-f3be-47c5-fa26-b3a579895bdd"
      },
      "source": [
        "vocab_size = len(t.word_index) # 단어 집합의 크기. 이 경우는 단어의 개수가 7이므로 7.\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "x = to_categorical(x, num_classes=vocab_size+1) # 실제 단어 집합의 크기보다 +1로 크기를 만들어야함.\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
